WEBVTT

1
00:00:00.000 --> 00:00:00.750 align:middle line:90%


2
00:00:00.750 --> 00:00:04.110 align:middle line:84%
Welcome to the first lesson
in our course, which presents

3
00:00:04.110 --> 00:00:07.280 align:middle line:90%
an overview of parallelism.

4
00:00:07.280 --> 00:00:08.735 align:middle line:84%
Parallelism is a
form of computing

5
00:00:08.735 --> 00:00:14.150 align:middle line:84%
that performs several steps, or
phases, on multiple processors

6
00:00:14.150 --> 00:00:16.290 align:middle line:90%
or processor cores.

7
00:00:16.290 --> 00:00:19.270 align:middle line:84%
The first phase
involves splitting,

8
00:00:19.270 --> 00:00:22.800 align:middle line:84%
which means to partition the
overall task to be performed

9
00:00:22.800 --> 00:00:25.600 align:middle line:90%
into multiple sub-tasks.

10
00:00:25.600 --> 00:00:30.340 align:middle line:84%
Ideally, these sub-tasks should
be split efficiently and evenly

11
00:00:30.340 --> 00:00:32.840 align:middle line:90%
from the original task.

12
00:00:32.840 --> 00:00:35.920 align:middle line:84%
The next phase is
the applying phase,

13
00:00:35.920 --> 00:00:39.190 align:middle line:84%
which involves running these
independent sub-tasks that

14
00:00:39.190 --> 00:00:42.250 align:middle line:84%
were created in the
split phase in parallel

15
00:00:42.250 --> 00:00:44.870 align:middle line:90%
on multiple processor cores.

16
00:00:44.870 --> 00:00:48.530 align:middle line:84%
Although each sub-task runs
sequentially on one core,

17
00:00:48.530 --> 00:00:52.090 align:middle line:84%
together they all
run in parallel.

18
00:00:52.090 --> 00:00:55.300 align:middle line:84%
The third and final phase
is the combining phase,

19
00:00:55.300 --> 00:00:58.900 align:middle line:84%
which involves merging the
sub-results from the sub-tasks

20
00:00:58.900 --> 00:01:03.470 align:middle line:84%
into a single so-called
reduced result.

21
00:01:03.470 --> 00:01:07.040 align:middle line:84%
A key goal of parallelism is
to efficiently partition tasks

22
00:01:07.040 --> 00:01:09.740 align:middle line:84%
into sub-tasks and
combine the results

23
00:01:09.740 --> 00:01:14.030 align:middle line:84%
in a way that works well
under different conditions.

24
00:01:14.030 --> 00:01:15.800 align:middle line:84%
You can therefore
view parallelism

25
00:01:15.800 --> 00:01:19.400 align:middle line:84%
as an optimization intended to
improve the overall performance

26
00:01:19.400 --> 00:01:21.018 align:middle line:90%
of the system.

27
00:01:21.018 --> 00:01:22.560 align:middle line:84%
When we talk about
performance, we're

28
00:01:22.560 --> 00:01:25.712 align:middle line:84%
really referring to three
characteristics-- throughput,

29
00:01:25.712 --> 00:01:28.170 align:middle line:84%
which is the amount of work
that can be done per unit time,

30
00:01:28.170 --> 00:01:31.020 align:middle line:84%
such as per second,
per minute, per week;

31
00:01:31.020 --> 00:01:33.720 align:middle line:84%
scalability, which is the
ability to take the work

32
00:01:33.720 --> 00:01:37.350 align:middle line:84%
and scale it up over a larger
number of cores elastically;

33
00:01:37.350 --> 00:01:40.320 align:middle line:84%
and latency, which is a measure
of the delay between when

34
00:01:40.320 --> 00:01:42.355 align:middle line:84%
the request comes in
for the work to do

35
00:01:42.355 --> 00:01:46.180 align:middle line:84%
and when the work is
finished being performed.

36
00:01:46.180 --> 00:01:48.990 align:middle line:84%
Let's now talk about when
to apply parallelism.

37
00:01:48.990 --> 00:01:51.150 align:middle line:84%
It turns out in
practice, parallelism

38
00:01:51.150 --> 00:01:54.510 align:middle line:84%
works best under
certain conditions.

39
00:01:54.510 --> 00:01:57.440 align:middle line:84%
First and foremost, it helps
if the tasks themselves

40
00:01:57.440 --> 00:01:59.330 align:middle line:90%
are independent.

41
00:01:59.330 --> 00:02:03.110 align:middle line:84%
These are often referred to as
embarrassingly parallel tasks,

42
00:02:03.110 --> 00:02:04.640 align:middle line:84%
which have little
or no dependency

43
00:02:04.640 --> 00:02:07.580 align:middle line:84%
or need for communication
with other tasks,

44
00:02:07.580 --> 00:02:09.259 align:middle line:84%
or don't need to
share the results

45
00:02:09.259 --> 00:02:12.440 align:middle line:84%
from the intermediate tasks
between the other tasks.

46
00:02:12.440 --> 00:02:15.650 align:middle line:84%
A good example of embarrassingly
parallel processing in practice

47
00:02:15.650 --> 00:02:17.960 align:middle line:84%
is taking your clothes
to a laundromat, where

48
00:02:17.960 --> 00:02:21.470 align:middle line:84%
you may have many different
washing machines or dryers that

49
00:02:21.470 --> 00:02:24.440 align:middle line:84%
can wash and dry your
clothes in parallel.

50
00:02:24.440 --> 00:02:27.320 align:middle line:84%
And obviously, one load of
laundry in one washing machine

51
00:02:27.320 --> 00:02:29.690 align:middle line:84%
has no dependencies on
the other loads of laundry

52
00:02:29.690 --> 00:02:32.022 align:middle line:90%
in the other machines.

53
00:02:32.022 --> 00:02:33.730 align:middle line:84%
Another condition
where parallelism works

54
00:02:33.730 --> 00:02:36.820 align:middle line:84%
well is when there's lots
of data and processing

55
00:02:36.820 --> 00:02:39.920 align:middle line:84%
to perform on the
multi-processor cores.

56
00:02:39.920 --> 00:02:41.750 align:middle line:84%
In fact, there's even
a mathematical model

57
00:02:41.750 --> 00:02:44.000 align:middle line:84%
to estimate the
improvements that you're

58
00:02:44.000 --> 00:02:46.370 align:middle line:84%
likely to see from using
parallel processing.

59
00:02:46.370 --> 00:02:48.040 align:middle line:84%
This is called the
N times Q model,

60
00:02:48.040 --> 00:02:49.790 align:middle line:84%
and you can read more
about it in the link

61
00:02:49.790 --> 00:02:51.540 align:middle line:90%
at the bottom of the slide.

62
00:02:51.540 --> 00:02:53.720 align:middle line:84%
In this model, N is a
number of data elements

63
00:02:53.720 --> 00:02:58.100 align:middle line:84%
that are being processed per
thread or per core, and so on.

64
00:02:58.100 --> 00:03:01.850 align:middle line:84%
Q is a quantification
of how CPU-intensive

65
00:03:01.850 --> 00:03:03.815 align:middle line:90%
any given processing step is.

66
00:03:03.815 --> 00:03:07.640 align:middle line:84%
And the idea of this model is
if N times Q is a large number,

67
00:03:07.640 --> 00:03:09.380 align:middle line:84%
you're going to
get a better payoff

68
00:03:09.380 --> 00:03:12.050 align:middle line:84%
from parallel processing,
whereas if N times Q is

69
00:03:12.050 --> 00:03:15.992 align:middle line:84%
a small number, you probably
won't get much of a speedup.

70
00:03:15.992 --> 00:03:18.200 align:middle line:84%
Yet another condition under
which parallel processing

71
00:03:18.200 --> 00:03:21.020 align:middle line:84%
works well is when threads
that are performing

72
00:03:21.020 --> 00:03:22.910 align:middle line:84%
the processing on
the underlying cores

73
00:03:22.910 --> 00:03:26.720 align:middle line:84%
neither block nor share
any mutable state.

74
00:03:26.720 --> 00:03:29.630 align:middle line:84%
And as a consequence, Java's
parallelism frameworks,

75
00:03:29.630 --> 00:03:31.880 align:middle line:84%
which are based on something
called the fork-join pool

76
00:03:31.880 --> 00:03:33.920 align:middle line:84%
that we'll talk
about later, tend

77
00:03:33.920 --> 00:03:37.010 align:middle line:84%
to try to avoid blocking
if at all possible.

78
00:03:37.010 --> 00:03:39.620 align:middle line:84%
And in fact, there's a concept
called work stealing that's

79
00:03:39.620 --> 00:03:41.110 align:middle line:84%
implemented by
the fork-join pool

80
00:03:41.110 --> 00:03:43.190 align:middle line:84%
that we'll also cover
later in this course, which

81
00:03:43.190 --> 00:03:46.040 align:middle line:84%
talks about how a thread
that's running on a processor

82
00:03:46.040 --> 00:03:49.400 align:middle line:84%
core that runs out of work
to do in its work queue

83
00:03:49.400 --> 00:03:51.080 align:middle line:84%
will look around and
try to steal work

84
00:03:51.080 --> 00:03:55.132 align:middle line:84%
from the ends of other threads
that have too much work to do,

85
00:03:55.132 --> 00:03:56.590 align:middle line:84%
with the goal of
trying to maximize

86
00:03:56.590 --> 00:03:59.877 align:middle line:84%
CPU utilization in
the overall system.

87
00:03:59.877 --> 00:04:01.460 align:middle line:84%
And then, of course,
another condition

88
00:04:01.460 --> 00:04:03.830 align:middle line:84%
under which processing
works well in parallel

89
00:04:03.830 --> 00:04:06.950 align:middle line:84%
is if there are many processors
and/or multiple processor

90
00:04:06.950 --> 00:04:07.880 align:middle line:90%
cores.

91
00:04:07.880 --> 00:04:10.100 align:middle line:84%
So obviously, the larger
the number of the processing

92
00:04:10.100 --> 00:04:12.890 align:middle line:84%
elements in the hardware that
could actually run in parallel,

93
00:04:12.890 --> 00:04:14.265 align:middle line:84%
the better the
payoff we're going

94
00:04:14.265 --> 00:04:16.450 align:middle line:84%
to see from using
parallel processing.