Now, that I've given you a high-level view of the search with parallel streams, class methods process stream and processing. Put, let's step in and visualize them a little bit more definitively.
 So we'll start by visualizing process stream. This of course searches, a list of input strings. What we get is input is in is a list of inputs trains that contain the works of Shakespeare.
 And then we convert that list of inputs trains into a parallel stream by calling the parallel stream Factor method and we'll end up of course, with sub streams with chunks of inputs trains in. In fact, each. Something has one input strand that gets processed because that's the way the splitter ater for arraylist works.
 What we have is the stream of input strings which will be processed in parallel and parallel on separate threads and cores as part of the fork, joint.
 The next thing that happens is will have those streams of strings processed by the map aggregate operation. And this of course, will call the process input method which will look at it in more detail on the 2nd, which is passed in is a method reference Behavior to the map in immediate operation and that will go off in search for phrases in each input string will look at that shortly. So, keep in mind, we will have parallelism going on in two Dimensions. Your powers. I'm at the level of each input string, each work of Shakespeare, and then parallelism at the level of each phrase that we're searching for.
 Now, what you get back from this map call will be a stream of lists of search results because that's what process input returns a list of search results. So have a stream of search results coming back and keep in mind that some of these lists of search results may be empty. If there weren't any phrases that matched, a given input during a given work or Shakespeare. Will talk more about how that gets dealt with later.
 That stream of list of search results gets then fed into the collect terminal operation, which of course triggers all the intermediate operation processing setting on the wheels in motion and that will actually run everything on the workers Fred's in the fork, join pool and that will, of course, be map to the underlined processor course. I want lots of parallelism cracking away.
 And what gets returned here will be a list of list of search results based on encounter order. So again, whatever the order that the works of Shakespeare were examined, that'll be the order in which the collected list of search results. Come back, some empty some perhaps not empty.
 This is. Of course, by the to list facts about that on the collectors utility class, which is you recall returns a collector or non concurrent collector, which works in a parallel stream as we'll see later and it's non concurrent collector. Will use an arraylist to accumulate the elements into the final beautiful result can container, which is the result from collect.
 Let's not talk about how to visualize the process input method. So this of course, is the method it's called is a kind of subroutine by process stream. Process. Input finds phrase is an input strain in parallel. It takes his input. The list of phrases to find and uses the Perla stream Factor method, to convert the list of strings into a stream of strings. So, we have a stream of strings, and all of these dreams, strings will then be processed in parallel in the comic book joint. Multiple threats in the mapping to the multiple course, just like process string. So get lots of parallelism going on in this example.
 In this particular case, the stream of phrases to find gets fed into the map, intermediate operation, which ends up using the search for phrase method, which we talked about before the search for phrases in a given input strength and that will output a stream of search results. And this time, we're going to go ahead and filter the streams search results to get rid of anything that happens to be empty. So if we end up with nothing, but empty, search results will end up with a list of empty search results. If we have, essentially any results at all will end up basically, let him pass through the filter.
 And then everything will be output as a stream of non, if you search results.
 Some, which may be empty. If there's no matches and will, then go ahead and collect everything together which as usual triggers, energy operations, and then collects the stream of search results into a list. So I can into a stream of result collected into a list, which is, of course, an arraylist, because we use the to list Factor method on the collectors utility class. And then we end up returning a list of search results, and it goes back to the thread that called this and it for a list of the ordered, an encounter order encounter order. Here means the order, in which the phrase has occurred in the input to process the input to Carol Stream.
 Animation before, if none of the phrases match, the input string will simply have an empty list.
 And that will be dealt with later by the flat map operation in Prince phases.
 Print phrases.
 So once again, just to recap varies important to recognize with parallel streams processing of a parallel stream, differs a little bit from the the visualization. I've been getting updates. I've been getting a visualization and props gives the impression that things are processed through the layer at a time. And while it's sort of like that, logically, that's not in fact how the application works. Instead what happens is, when the terminal operation is, reached the stream will start pulling data from the source and it true versus through all of the intermediate operations in the Stream. Working on that date a lot. Of course, we will be run within the common for joint pool within a joint. Using. What's a integrated laser processing to pull the data through all the various intermediate operations. And then of course, ultimately return of operation will end up popping the result into the appropriate collect.
 And Dad's freezing collect and it'll go ahead and collect the results. Using the accumulator. The reason again. Why it's done? This way. There's two reasons. Everyone. It supports short-circuit operations, and number two. It's much much more efficient. Because we don't have to incur unnecessary contact switching synchronization memory management and cash management. Overhead works very smoothly, very cleanly.